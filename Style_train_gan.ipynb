{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mB_sFildiDh"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_train_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "303iHntmdiDj"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "**Module 7: Generative Adversarial Networks**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5n2iv9udiDk"
      },
      "source": [
        "# Module 7 Material\n",
        "\n",
        "* Part 7.1: Introduction to GANs for Image and Data Generation [[Video]](https://www.youtube.com/watch?v=hZw-AjbdN5k&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_1_gan_intro.ipynb)\n",
        "* **Part 7.2: Train StyleGAN3 with your Own Images** [[Video]](https://www.youtube.com/watch?v=R546LYsQk5M&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_train_gan.ipynb)\n",
        "* Part 7.3: Exploring the StyleGAN Latent Vector [[Video]](https://www.youtube.com/watch?v=goQzp8QSb2s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_3_latent_vector.ipynb)\n",
        "* Part 7.4: GANs to Enhance Old Photographs Deoldify [[Video]](https://www.youtube.com/watch?v=0OTd5GlHRx4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_4_deoldify.ipynb)\n",
        "* Part 7.5: GANs for Tabular Synthetic Data Generation [[Video]](https://www.youtube.com/watch?v=yujdA46HKwA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_5_tabular_synthetic.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ohl-hxAAaNp"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8_-1h5ddiDp",
        "outputId": "76c25a8f-021d-4f5a-e187-7a9c17274bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yqlUD4sdiDk"
      },
      "source": [
        "# Part 7.2: Train StyleGAN3 with your Images\n",
        "\n",
        "Training GANs with StyleGAN is resource-intensive. The NVIDA StyleGAN researchers used computers with eight high-end GPUs for the high-resolution face GANs trained by NVIDIA. The GPU used by NVIDIA is an A100, which has more memory and cores than the P100 or V100 offered by even Colab Pro+. In this part, we will use StyleGAN2 to train rather than StyleGAN3. You can use networks trained with StyleGAN2 from StyleGAN3; however, StyleGAN3 usually is more effective at training than StyleGAN2.\n",
        "\n",
        "Unfortunately, StyleGAN3 is compute-intensive and will perform slowly on any GPU that is not the latest Ampere technology. Because Colab does not provide such technology, I am keeping the training guide at the StyleGAN2 level. Switching to StyleGAN3 is relatively easy, as will be pointed out later. \n",
        "\n",
        "Make sure that you are running this notebook with a GPU runtime. You can train GANs with either Google Colab Free or Pro. I recommend at least the Pro version due to better GPU instances, longer runtimes, and timeouts. Additionally, the capability of Google Colab Pro to run in the background is valuable when training GANs, as you can close your browser or reboot your laptop while training continues.\n",
        "\n",
        "\n",
        "\n",
        "You will store your training data and trained neural networks to GDRIVE. For GANs, I lay out my GDRIVE like this:\n",
        "\n",
        "* ./data/gan/images - RAW images I wish to train on.\n",
        "* ./data/gan/datasets - Actual training datasets that I convert from the raw images.\n",
        "* ./data/gan/experiments - The output from StyleGAN2, my image previews, and saved network snapshots.\n",
        "\n",
        "You will mount the drive at the following location.\n",
        "\n",
        "```\n",
        "/content/drive/MyDrive/data\n",
        "```\n",
        "\n",
        "## What Sort of GPU do you Have?\n",
        "\n",
        "The type of GPU assigned to you by Colab will significantly affect your training time. Some sample times that I achieved with Colab are given here. I've found that Colab Pro generally starts you with a V100, however, if you run scripts non-stop for 24hrs straight for a few days in a row, you will generally be throttled back to a P100.\n",
        "\n",
        "* 1024x1024 - V100 - 566 sec/tick (CoLab Pro)\n",
        "* 1024x1024 - P100 - 1819 sec/tick (CoLab Pro)\n",
        "* 1024x1024 - T4 - 2188 sec/tick (CoLab Free)\n",
        "\n",
        "By comparison, a 1024x1024 GAN trained with StyleGAN3 on a V100 is 3087 sec/tick.\n",
        "\n",
        "If you use Google CoLab Pro, generally, it will not disconnect before 24 hours, even if you (but not your script) are inactive. Free CoLab WILL disconnect a perfectly good running script if you do not interact for a few hours. The following describes how to circumvent this issue.\n",
        "\n",
        "* [How to prevent Google Colab from disconnecting?](https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting)\n",
        "\n",
        "## Set Up New Environment\n",
        "\n",
        "You will likely need to train for >24 hours. Colab will disconnect you. You must be prepared to restart training when this eventually happens. Training is divided into ticks, every so many ticks (50 by default), your neural network is evaluated, and a snapshot is saved. When CoLab shuts down, all training after the last snapshot is lost. It might seem desirable to snapshot after each tick; however, this snapshotting process itself takes nearly an hour. Learning an optimal snapshot size for your resolution and training data is important.\n",
        "\n",
        "We will mount GDRIVE so that you will save your snapshots there. You must also place your training images in GDRIVE.\n",
        "\n",
        "You must also install NVIDIA StyleGAN2 ADA PyTorch. We also need to downgrade PyTorch to a version that supports StyleGAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LVr5sV8CKz0",
        "outputId": "80b90b61-2575-4e82-db3a-29b5e73117ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: jax 0.4.4\n",
            "Uninstalling jax-0.4.4:\n",
            "  Successfully uninstalled jax-0.4.4\n",
            "Found existing installation: jaxlib 0.4.4+cuda11.cudnn82\n",
            "Uninstalling jaxlib-0.4.4+cuda11.cudnn82:\n",
            "  Successfully uninstalled jaxlib-0.4.4+cuda11.cudnn82\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting jax[cuda11_cudnn805]==0.3.10\n",
            "  Downloading jax-0.3.10.tar.gz (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.7/939.7 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (1.22.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.9/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (1.10.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (4.5.0)\n",
            "Collecting jaxlib==0.3.10+cuda11.cudnn805\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.10%2Bcuda11.cudnn805-cp39-none-manylinux2014_x86_64.whl (175.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.7/175.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.3.10-py3-none-any.whl size=1088070 sha256=3be3da3f1965373dafec5f1818fe981b0abec99d2a58b60201b6f388aab2686d\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/4a/ff/e9ddfa09012c67d22f926a7873c546c04e722969e8d86f84ec\n",
            "Successfully built jax\n",
            "Installing collected packages: flatbuffers, jaxlib, jax\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "Successfully installed flatbuffers-2.0.7 jax-0.3.10 jaxlib-0.3.10+cuda11.cudnn805\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp39-cp39-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.1/804.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.9.1\n",
            "  Downloading torchvision-0.9.1-cp39-cp39-manylinux1_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch==1.8.1) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.8.1) (4.5.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.9.1) (8.4.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.8.1 torchvision-0.9.1\n",
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 24.42 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall jax jaxlib -y\n",
        "!pip install \"jax[cuda11_cudnn805]==0.3.10\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install torch==1.8.1 torchvision==0.9.1\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBwmbfKJCORs"
      },
      "source": [
        "## Find Your Files\n",
        "\n",
        "The drive is mounted to the following location.\n",
        "\n",
        "```\n",
        "/content/drive/MyDrive/data\n",
        "```\n",
        "\n",
        "It might be helpful to use an ```ls``` command to establish the exact path for your images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H7WrHDJyarIP",
        "outputId": "82cfeb98-98ea-4810-9e06-a00fd19343f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3f3U7rebbgj",
        "outputId": "df66e0f1-107f-414c-fdb4-d4fc28c83fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mstylegan2-ada-pytorch\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spYgtQeBbgsB",
        "outputId": "e7084759-96ea-4420-e77f-96a79b7ff2f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'Real_Images_Cropped/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd Real_Images_Cropped/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwpXtlrCCRHe",
        "outputId": "2ddae246-fa15-4e2a-95db-069f134f9fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frame100.jpg  frame138.jpg  frame175.jpg  frame2.jpg   frame67.jpg\n",
            "frame101.jpg  frame139.jpg  frame176.jpg  frame30.jpg  frame68.jpg\n",
            "frame102.jpg  frame13.jpg   frame177.jpg  frame31.jpg  frame69.jpg\n",
            "frame103.jpg  frame140.jpg  frame178.jpg  frame32.jpg  frame6.jpg\n",
            "frame104.jpg  frame141.jpg  frame179.jpg  frame33.jpg  frame70.jpg\n",
            "frame105.jpg  frame142.jpg  frame17.jpg   frame34.jpg  frame71.jpg\n",
            "frame106.jpg  frame143.jpg  frame180.jpg  frame35.jpg  frame72.jpg\n",
            "frame107.jpg  frame144.jpg  frame181.jpg  frame36.jpg  frame73.jpg\n",
            "frame108.jpg  frame145.jpg  frame182.jpg  frame37.jpg  frame74.jpg\n",
            "frame109.jpg  frame146.jpg  frame183.jpg  frame38.jpg  frame75.jpg\n",
            "frame10.jpg   frame147.jpg  frame184.jpg  frame39.jpg  frame76.jpg\n",
            "frame110.jpg  frame148.jpg  frame185.jpg  frame3.jpg   frame77.jpg\n",
            "frame111.jpg  frame149.jpg  frame186.jpg  frame40.jpg  frame78.jpg\n",
            "frame112.jpg  frame14.jpg   frame187.jpg  frame41.jpg  frame79.jpg\n",
            "frame113.jpg  frame150.jpg  frame188.jpg  frame42.jpg  frame7.jpg\n",
            "frame114.jpg  frame151.jpg  frame189.jpg  frame43.jpg  frame80.jpg\n",
            "frame115.jpg  frame152.jpg  frame18.jpg   frame44.jpg  frame81.jpg\n",
            "frame116.jpg  frame153.jpg  frame190.jpg  frame45.jpg  frame82.jpg\n",
            "frame117.jpg  frame154.jpg  frame191.jpg  frame46.jpg  frame83.jpg\n",
            "frame118.jpg  frame155.jpg  frame192.jpg  frame47.jpg  frame84.jpg\n",
            "frame119.jpg  frame156.jpg  frame193.jpg  frame48.jpg  frame85.jpg\n",
            "frame11.jpg   frame157.jpg  frame194.jpg  frame49.jpg  frame86.jpg\n",
            "frame120.jpg  frame158.jpg  frame195.jpg  frame4.jpg   frame87.jpg\n",
            "frame121.jpg  frame159.jpg  frame196.jpg  frame50.jpg  frame88.jpg\n",
            "frame122.jpg  frame15.jpg   frame197.jpg  frame51.jpg  frame89.jpg\n",
            "frame123.jpg  frame160.jpg  frame198.jpg  frame52.jpg  frame8.jpg\n",
            "frame124.jpg  frame161.jpg  frame199.jpg  frame53.jpg  frame90.jpg\n",
            "frame125.jpg  frame162.jpg  frame19.jpg   frame54.jpg  frame91.jpg\n",
            "frame126.jpg  frame163.jpg  frame1.jpg\t  frame55.jpg  frame92.jpg\n",
            "frame127.jpg  frame164.jpg  frame200.jpg  frame56.jpg  frame93.jpg\n",
            "frame128.jpg  frame165.jpg  frame201.jpg  frame57.jpg  frame94.jpg\n",
            "frame129.jpg  frame166.jpg  frame20.jpg   frame58.jpg  frame95.jpg\n",
            "frame12.jpg   frame167.jpg  frame21.jpg   frame59.jpg  frame96.jpg\n",
            "frame130.jpg  frame168.jpg  frame22.jpg   frame5.jpg   frame97.jpg\n",
            "frame131.jpg  frame169.jpg  frame23.jpg   frame60.jpg  frame98.jpg\n",
            "frame132.jpg  frame16.jpg   frame24.jpg   frame61.jpg  frame99.jpg\n",
            "frame133.jpg  frame170.jpg  frame25.jpg   frame62.jpg  frame9.jpg\n",
            "frame134.jpg  frame171.jpg  frame26.jpg   frame63.jpg\n",
            "frame135.jpg  frame172.jpg  frame27.jpg   frame64.jpg\n",
            "frame136.jpg  frame173.jpg  frame28.jpg   frame65.jpg\n",
            "frame137.jpg  frame174.jpg  frame29.jpg   frame66.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Real_Images_Cropped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl6ShUWrCUrV"
      },
      "source": [
        "## Convert Your Images\n",
        "\n",
        "You must convert your images into a data set form that PyTorch can directly utilize. The following command converts your images and writes the resulting data set to another directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOK8m0N-Caqa",
        "outputId": "35bdb246-9ce5-4258-c0e2-ee3bc4f1e893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 201/201 [00:03<00:00, 52.26it/s]\n"
          ]
        }
      ],
      "source": [
        "CMD = \"python /content/stylegan2-ada-pytorch/dataset_tool.py \"\\\n",
        "  \"--source /content/drive/MyDrive/Real_Images_Cropped \"\\\n",
        "  \"--dest /content/drive/MyDrive/data/gan/dataset\"\n",
        "\n",
        "!{CMD}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDbsiP35CdgK"
      },
      "source": [
        "You can use the following command to clear out the newly created dataset.  If something goes wrong and you need to clean up your images and rerun the above command, you should delete your partially completed dataset directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpF072wiCgys"
      },
      "outputs": [],
      "source": [
        "#!rm -R /content/drive/MyDrive/data/gan/dataset/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7sUkkb-CkrS"
      },
      "source": [
        "## Clean Up your Images\n",
        "\n",
        "All images must have the same dimensions and color depth.  This code can identify images that have issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c646ac854f5649b2a6d2ee533503b128",
            "bc3d0ac2a09d4ba7aef9af08ddabb94d",
            "f7b871d24bd44fdc9603f1f9acf49cb4",
            "5a7837903bc54b0ca1e93eadae547763",
            "ade49b3b3c13422192d5a3cf94969376",
            "35e4de0d47f64c81bc7b3df980070609",
            "3cbc56c9133440b7a4959bbc12e8ffcf",
            "7445ad5ee53447009e2184f9fe2d4dd5",
            "1a4e66179b1e434aad1bc340827cccf4",
            "1cc4b45b5fea4f78868e3bd543898470",
            "a32bb8db66c1472f87e3311ac26b24e2"
          ]
        },
        "id": "QuWfEfEoColz",
        "outputId": "0c28697e-d70e-47c9-a12f-ca865cda5048"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c646ac854f5649b2a6d2ee533503b128",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/201 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "IMAGE_PATH = '/content/drive/MyDrive/Real_Images_Cropped'\n",
        "files = [f for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))]\n",
        "\n",
        "base_size = None\n",
        "for file in tqdm(files):\n",
        "  file2 = os.path.join(IMAGE_PATH,file)\n",
        "  img = Image.open(file2)\n",
        "  sz = img.size\n",
        "  if base_size and sz!=base_size:\n",
        "    print(f\"Inconsistant size: {file2}\")\n",
        "  elif img.mode!='RGB':\n",
        "    print(f\"Inconsistant color format: {file2}\")\n",
        "  else:\n",
        "    base_size = sz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HyFZgnjCsPb"
      },
      "source": [
        "## Perform Initial Training\n",
        "\n",
        "This code performs the initial training.  Set SNAP low enough to get a snapshot before Colab forces you to quit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-ZBcql7Cv14",
        "outputId": "844641f0-b1ef-4989-a0fa-09ae5f3c47f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 1,\n",
            "  \"network_snapshot_ticks\": 1,\n",
            "  \"metrics\": [\n",
            "    \"fid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/data/gan/dataset\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 201,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 0.8192\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 16,\n",
            "  \"batch_gpu\": 16,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"ema_rampup\": 0.05,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"run_dir\": \"/content/drive/MyDrive/data/gan/experiments/00001-dataset-auto1\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/data/gan/experiments/00001-dataset-auto1\n",
            "Training data:      /content/drive/MyDrive/data/gan/dataset\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   201\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "Num images:  201\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape         Datatype\n",
            "---                   ---         ---      ---                  ---     \n",
            "mapping.fc0           262656      -        [16, 512]            float32 \n",
            "mapping.fc1           262656      -        [16, 512]            float32 \n",
            "mapping               -           512      [16, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [16, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [16, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [16, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [16, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [16, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [16, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [16, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [16, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [16, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [16, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [16, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [16, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [16, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [16, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [16, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [16, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [16, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [16, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [16, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                  ---     \n",
            "Total                 23191522    175568   -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   256         16       [16, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [16, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [16, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [16, 128, 128, 128]  float16 \n",
            "b256           -           16       [16, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [16, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [16, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [16, 256, 64, 64]    float16 \n",
            "b128           -           16       [16, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [16, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [16, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [16, 512, 32, 32]    float16 \n",
            "b64            -           16       [16, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [16, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [16, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [16, 512, 16, 16]    float16 \n",
            "b32            -           16       [16, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [16, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [16, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [16, 512, 8, 8]      float32 \n",
            "b16            -           16       [16, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [16, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [16, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [16, 512, 4, 4]      float32 \n",
            "b8             -           16       [16, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [16, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [16, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [16, 512]            float32 \n",
            "b4.out         513         -        [16, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          24001089    416      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "2023-03-16 04:40:38.337402: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-16 04:40:41.582512: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-16 04:40:41.582906: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-16 04:40:41.582936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 48s       sec/tick 10.9    sec/kimg 684.30  maintenance 96.7   cpumem 4.13   gpumem 11.17  augment 0.000\n",
            "Evaluating metrics...\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{\"results\": {\"fid50k_full\": 386.31476920967697}, \"metric\": \"fid50k_full\", \"total_time\": 645.5509991645813, \"total_time_str\": \"10m 46s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1678942317.8795805}\n",
            "tick 1     kimg 4.0      time 17m 25s      sec/tick 275.1   sec/kimg 68.78   maintenance 662.4  cpumem 4.72   gpumem 9.39   augment 0.006\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 521.0840139915526}, \"metric\": \"fid50k_full\", \"total_time\": 640.3601853847504, \"total_time_str\": \"10m 40s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000004.pkl\", \"timestamp\": 1678943250.9387276}\n",
            "tick 2     kimg 8.0      time 32m 59s      sec/tick 276.1   sec/kimg 69.02   maintenance 658.0  cpumem 4.85   gpumem 4.87   augment 0.012\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 474.8632491869436}, \"metric\": \"fid50k_full\", \"total_time\": 638.43146443367, \"total_time_str\": \"10m 38s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000008.pkl\", \"timestamp\": 1678944182.2367547}\n",
            "tick 3     kimg 12.0     time 48m 30s      sec/tick 275.9   sec/kimg 68.97   maintenance 655.2  cpumem 4.97   gpumem 4.93   augment 0.018\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 479.14506199219034}, \"metric\": \"fid50k_full\", \"total_time\": 632.2209606170654, \"total_time_str\": \"10m 32s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000012.pkl\", \"timestamp\": 1678945106.5791452}\n",
            "tick 4     kimg 16.0     time 1h 03m 56s   sec/tick 277.0   sec/kimg 69.24   maintenance 648.5  cpumem 4.94   gpumem 4.90   augment 0.024\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 433.04120383137797}, \"metric\": \"fid50k_full\", \"total_time\": 636.4957811832428, \"total_time_str\": \"10m 36s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000016.pkl\", \"timestamp\": 1678946046.4267926}\n",
            "tick 5     kimg 20.0     time 1h 19m 35s   sec/tick 276.2   sec/kimg 69.04   maintenance 662.9  cpumem 4.92   gpumem 4.88   augment 0.032\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 406.6348359175846}, \"metric\": \"fid50k_full\", \"total_time\": 634.8251502513885, \"total_time_str\": \"10m 35s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000020.pkl\", \"timestamp\": 1678946977.9752514}\n",
            "tick 6     kimg 24.0     time 1h 35m 07s   sec/tick 276.5   sec/kimg 69.13   maintenance 655.4  cpumem 4.86   gpumem 4.87   augment 0.039\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 406.3944672415244}, \"metric\": \"fid50k_full\", \"total_time\": 631.4016463756561, \"total_time_str\": \"10m 31s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000024.pkl\", \"timestamp\": 1678947904.8231199}\n",
            "tick 7     kimg 28.0     time 1h 50m 34s   sec/tick 277.0   sec/kimg 69.24   maintenance 650.3  cpumem 5.01   gpumem 4.96   augment 0.046\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 415.95716092169545}, \"metric\": \"fid50k_full\", \"total_time\": 635.0451407432556, \"total_time_str\": \"10m 35s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000028.pkl\", \"timestamp\": 1678948840.9195657}\n",
            "tick 8     kimg 32.0     time 2h 06m 11s   sec/tick 277.4   sec/kimg 69.35   maintenance 659.1  cpumem 5.02   gpumem 4.90   augment 0.053\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 418.7452994098176}, \"metric\": \"fid50k_full\", \"total_time\": 631.4488985538483, \"total_time_str\": \"10m 31s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000032.pkl\", \"timestamp\": 1678949765.7366152}\n",
            "tick 9     kimg 36.0     time 2h 21m 35s   sec/tick 277.3   sec/kimg 69.34   maintenance 647.4  cpumem 4.98   gpumem 4.89   augment 0.060\n",
            "Evaluating metrics...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Modify these to suit your needs\n",
        "EXPERIMENTS = \"/content/drive/MyDrive/data/gan/experiments\"\n",
        "DATA = \"/content/drive/MyDrive/data/gan/dataset\"\n",
        "SNAP = 1\n",
        "\n",
        "# Build the command and run it\n",
        "cmd = f\"/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py \"\\\n",
        "  f\"--snap {SNAP} --outdir {EXPERIMENTS} --data {DATA}\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhOQRZrJDCvA"
      },
      "source": [
        "## Resume Training\n",
        "\n",
        "You can now resume training after you are interrupted by something in the pervious step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1TFgr9MDJlS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Modify these to suit your needs\n",
        "EXPERIMENTS = \"/content/drive/MyDrive/data/gan/experiments\"\n",
        "NETWORK = \"network-snapshot-000100.pkl\"\n",
        "RESUME = os.path.join(EXPERIMENTS, \\\n",
        "                \"00008-circuit-auto1-resumecustom\", NETWORK)\n",
        "DATA = \"/content/drive/MyDrive/data/gan/dataset/circuit\"\n",
        "SNAP = 10\n",
        "\n",
        "# Build the command and run it\n",
        "cmd = f\"/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py \"\\\n",
        "  f\"--snap {SNAP} --resume {RESUME} --outdir {EXPERIMENTS} --data {DATA}\"\n",
        "!{cmd}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a4e66179b1e434aad1bc340827cccf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cc4b45b5fea4f78868e3bd543898470": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35e4de0d47f64c81bc7b3df980070609": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cbc56c9133440b7a4959bbc12e8ffcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a7837903bc54b0ca1e93eadae547763": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cc4b45b5fea4f78868e3bd543898470",
            "placeholder": "​",
            "style": "IPY_MODEL_a32bb8db66c1472f87e3311ac26b24e2",
            "value": " 201/201 [00:00&lt;00:00, 612.28it/s]"
          }
        },
        "7445ad5ee53447009e2184f9fe2d4dd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32bb8db66c1472f87e3311ac26b24e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ade49b3b3c13422192d5a3cf94969376": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc3d0ac2a09d4ba7aef9af08ddabb94d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35e4de0d47f64c81bc7b3df980070609",
            "placeholder": "​",
            "style": "IPY_MODEL_3cbc56c9133440b7a4959bbc12e8ffcf",
            "value": "100%"
          }
        },
        "c646ac854f5649b2a6d2ee533503b128": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc3d0ac2a09d4ba7aef9af08ddabb94d",
              "IPY_MODEL_f7b871d24bd44fdc9603f1f9acf49cb4",
              "IPY_MODEL_5a7837903bc54b0ca1e93eadae547763"
            ],
            "layout": "IPY_MODEL_ade49b3b3c13422192d5a3cf94969376"
          }
        },
        "f7b871d24bd44fdc9603f1f9acf49cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7445ad5ee53447009e2184f9fe2d4dd5",
            "max": 201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a4e66179b1e434aad1bc340827cccf4",
            "value": 201
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}